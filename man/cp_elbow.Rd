% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cp_elbow.R
\name{cp_elbow}
\alias{cp_elbow}
\title{Find optimal cp parameter using elbow method}
\usage{
cp_elbow(rpart_fit, cp_num = 10)
}
\arguments{
\item{rpart_fit}{An object of class rpart}

\item{cp_num}{How many cp values to evaluate}
}
\value{
optimal cp. It also plots cp and elbow strengh
}
\description{
The function uses elbow strength to find optimal cp
}
\details{
This function uses the method found
\href{https://www.datasciencecentral.com/profiles/blogs/how-to-automatically-determine-the-number-of-clusters-in-your-dat}{here}
to determine the optimal cp. It also plots cp and elbow strength to allow the user to a manual determination.
}
\examples{
set.seed(3) # vary this and the sample size to get a sense of the model performance
library(segmenTree)

# Generate a dataset
p_x <- function(Tr, X1, X2, X3){
  lp <- 2*X1 + 0.2*X2 + as.numeric(X3)/6
  0.25 + 2*Tr*X1^3 + 0.5*exp(lp)/(1+exp(lp))
}

n <- 10000
Tr <- rbinom(n, 1, 0.3)
X1 <- runif(n, -0.5, 0.5)
X2 <- rnorm(n)
X3 <- factor(sample(LETTERS[1:3], size = n, replace = T))
p <- p_x(Tr, X1, X2, X3)
y <- sapply(p, function(x) rbinom(1, 1, x))
y_mat <- cbind(y, Tr)
dat <- data.frame(y = I(y_mat), X1, X2, X3)

# Fit a causal tree
lift_method <- import_lift_method()
segment_tree <- rpart(y ~ ., data = dat,
              method = lift_method,
              control = rpart.control(cp = 0),
              parms = list(), y = T, x = T)

# find optimal cp and train final model - still WIP
## Use elbow method on the cp itself
cp_vec <- segment_tree$cptable[1:10, 1]
plot(cp_vec)

dd <- data.frame(i = 1:length(cp_vec), cp = cp_vec)
dd$delta1 <- c(NA, dd$cp[1:9] - dd$cp[2:10])
dd$delta2 <- c(NA, NA, delta_1[1:8] - delta_1[2:9])
dd$elbow_strengh <- c((dd$delta2 - dd$delta1)[-1], NA)

plot(dd$i, dd$elbow_strengh)
optimal_cp
## Use tune_cp function
# cp_lift <- tune_cp(segment_tree, cp_num = 6, train_frac = 0.8, M = 100)
# optimal_cp <- as.numeric(names(which.max(apply(cp_lift, 2, mean)/apply(cp_lift, 2, sd))))

segment_tree2 <- prune(segment_tree, cp = cp_vec[2])


# Predict treatment effect and compare with actual treatment effect
tau <- predict(segment_tree2, dat)
p_treat <- p_x(rep(1, n), X1, X2, X3)
p_cont <- p_x(rep(0, n), X1, X2, X3)
cate <- p_treat - p_cont

# par(mfrow = c(1, 3))
y_lim <- c(min(tau, cate), max(tau, cate))
plot(c(min(dat$X1), max(dat$X1)), y_lim, type = "n", main = "segmenTree",
     xlab = "X1", ylab = "true (red) vs predicted (black) lift")
points(dat$X1, cate, col = "red")
points(dat$X1, tau)

# explore the resulting segments
segments <- extract_segments(segment_tree, alpha = 0.15)
# print(segments)

# Compare to a regular classfication model
dat2 <- data.frame(y, X1, X2, X3, Tr)
fit2 <- rpart(y ~ ., data = dat2, control = rpart.control(cp = 0))
fit2 <- rpart(y ~ ., data = dat,
                     method = lift_method,
                     control = rpart.control(cp = fit2$cptable[max(which(fit2$cptable[, 2] < 3)), 1]),
                     parms = list())

dat2_treat <- dat2; dat2_cont <- dat2
dat2_treat$Tr <- 1L; dat2_cont$Tr <- 0L
tau2 <- predict(fit2, dat2_treat) - predict(fit2, dat2_cont)

y_lim <- c(min(tau2, cate), max(tau2, cate))
plot(c(min(dat$X1), max(dat$X1)), y_lim, type = "n", main = "regular model",
     xlab = "X1", ylab = "")
points(dat$X1, cate, col = "red")
points(dat$X1, tau2)


# Compare to training seperately on control and treatment
dat3_treat <- dat2[dat2$Tr == 1, -5]
dat3_cont <- dat2[dat2$Tr == 0, -5]

fit3_treat <- rpart(y ~ ., data = dat3_treat)
fit3_cont <- rpart(y ~ ., data = dat3_cont)
dat2_treat <- dat2; dat2_cont <- dat2
tau3 <- predict(fit3_treat, dat2) - predict(fit3_cont, dat2)

y_lim <- c(min(tau3, cate), max(tau3, cate))
plot(c(min(dat$X1), max(dat$X1)), y_lim, type = "n", main = "seperate models",
     xlab = "X1", ylab = "")
points(dat$X1, cate, col = "red")
points(dat$X1, tau3)
}
\seealso{
\code{\link{tune_cp}}
}
